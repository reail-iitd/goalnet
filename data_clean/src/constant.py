import pickle
import os

verbose = True

if os.name == 'nt':
    root_dir = os.path.join(os.path.dirname(__file__))+"\\"
    #root_dir = "D:\\Users\\shelb\\Dropbox\\EclipseWorkSpacePython\\HypoSpaceActionLearningV2\\"
    data_dir = root_dir+"data\\"
    result_files_dir = root_dir+"result_files\\"
    
    precomputed_dataset_dir = root_dir+"premade_datasets/"
    precomputed_clausedata_fname = "data_byclause.p"
    precomputed_filedata_fname = "data_byfile.p"
    
    domain_file = root_dir+"symbplan\\domainKnowledge.pddl"
    tmp_problem_file = root_dir+"symbplan\\tmp_problem.pddl"
    # planner = root_dir+"symbplan\\Mp.exe"
    planner = root_dir+"symbplan\\MpC"
elif os.name == 'posix':
    root_dir = os.path.join(os.path.dirname(__file__))+"/"
    #root_dir = "/home/lair/shelb/HypoSpaceActionLearningV2/"
    data_dir = root_dir+"data/"
    result_files_dir = root_dir+"result_files/"

    precomputed_dataset_dir = root_dir#+"HypoSpaceActionLearning/"
    precomputed_clausedata_fname = "data_byclause.p"
    precomputed_filedata_fname = "data_byfile.p"
    
    domain_file = root_dir+"symbplan/domainKnowledge.pddl"
    tmp_problem_file = root_dir+"symbplan/tmp_problem.pddl"
    planner = root_dir+"symbplan/MpC"    
else:
    raise Exception('unknown os.name')
    
"""using planning result"""
use_cached_planning_result = True
cached_plan_file = 'symbplan_result_cache.p'
cached_planning_result = pickle.load(open(precomputed_dataset_dir+cached_plan_file, "rb")) 

def get_cached_planning_result():
    return cached_planning_result
def append_new_planning_result(prob_str, inst_seq):
    cached_planning_result[prob_str] = inst_seq
def dump_planning_result():
    pickle.dump(cached_planning_result, open(precomputed_dataset_dir+cached_plan_file, 'wb'))

    
"""raw data related"""
use_precomputed_data = True

"""space pruning options"""
# whether prune inconsistent nodes during space induction
# 'prune': prune nodes
# 'nprune': leave all the nodes, without pruning
space_pruning = 'prune'
    
"""use the data by file or by clause"""
use_data_by_file = False

"""load precomputed knowledge base"""
load_existing_kb = False

"""heuristics of building the bottom nodes""" 
# Heuristic1: the states of the argument objects only.
# Heuristic2: the states of all the objects that have been operated in the teaching process.
# Heuristic3: the states of all the objects that have direct relation with the argument objects.
# Heuristic4: all the states that have been changed   
node_chosen_heu = "Heuristic4"    
    
# The criteria used to generate the action sequence for a given testing data.
# Specifically, the testing action frame and initial environment will be sent
# to the knowledge base to generate action sequence. This crit defines how to 
# use the hypo space to generate action sequence and final environment.
# memory_based: only use the applicable bottom nodes to generate. These
#                   bottom nodes are exactly the goal state captured from induction set.
# most_general_node: find and use the most general hypotheses that are applicable.
# most_specific_node: find and use the most specific hypotheses that are applicable.
# highest_node_freq: find and use the hypotheses with highest frequency that are applicable
# highest_freq_actionseq: final and use the highest frequency action sequence
# upper_bound: upper bound of the current hypotheses space
# with_optimizer: with regression based optimizer
# with_optimizer_classificationbased: formulate the optimizer as a binary classifier
test_actseq_gen_crit = 'with_optimizer'    


# opm_type (regression based): choose from
#    sgd (using sgd regression, support incremental learning)
#    svr (using support vector regression)
# opm_type (classification based): choose from
#    sgd_lsvm (using sgd, hinge loss, equivalent to linear svm)
#    sgd_logreg (using sgd, logistic regression)
#    sgd_perceptron (using sgd, perceptron)
opm_type = 'svr'

# whether check the affordances when trying to match the testing action with existing 
# bottom nodes in the hypotheses space.
check_affords = False
    
epsilon = 0.000001
fold_num = 5
folder_id = 0
dev_portion = 5 #divide the train set by this number, and use the large fold for dev

"""
=========================================
    Following parameters are seldom used
=========================================
"""

use_changed_effects = True

# criteria used to check whether two action sequences are the same when building the hypo net.
# Crit1: the two action sequences have to be exactly the same, order of instructions the same, 
#        verb the same, arguments the same.
same_seq_crit = "Crit1" 

# criteria used to merge two hypo spaces.
# same_node_only: only merges nodes that are exactly the same, and records edges in/out this node in both spaces.
#                 so that ignores the situation where node i (ni) is different from any node in the other space (s2) 
#                 (the same level l) but could be generated by node in level l-1 from s2.
# check_links_cross_net: not only check the same node, but also check whether additional links should be added
#                        to link nodes between two nets.
space_merg_crit = 'check_links_cross_net'

# the criteria used to make the decision that whether two action frames are the same
# same_verb_same_argnum: the two action frames should use the same verb, and they
#                        have the same number of arguments. But there's no further
#                        specification for the relations/affordances of the arguments.
same_verbstrc_crit = 'same_verb_same_argnumb'

precomputed_kb = ('precomputed_kb\\byfile' if use_data_by_file else 'precomputed_kb\\bycls')+'+'+\
                 str(fold_num)+'folder'+'+test_fold'+str(folder_id)+'+'+\
                 node_chosen_heu+'+'+\
                 ('changedstate' if use_changed_effects else 'state')+'+'+\
                 same_seq_crit+'+'+space_merg_crit+'+'+same_verbstrc_crit+'+kb.p'
#based on the knowledge base, the system may generate more than one action 
#sequence. This crit decides which sequence to use.
# left_longest: use the left most longest action sequence and its corresponding
#               result.
multi_result_usage = 'left_longest'

# evaluation matrix
evl_matrix = ['LevenDistance']